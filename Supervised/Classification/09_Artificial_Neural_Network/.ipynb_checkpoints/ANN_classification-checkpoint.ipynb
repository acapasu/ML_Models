{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59175c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d57ccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    }
   ],
   "source": [
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d83415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c0411",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3258272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'logs\\\\fit_' + datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M\")\n",
    "board = TensorBoard(log_dir=log_directory,histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e55a5d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\andre\\\\Documents\\\\ML_Models\\\\Supervised\\\\Classification\\\\09_Artificial_Neural_Network'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeac1f6",
   "metadata": {},
   "source": [
    "tensorboard --logdir logs\\fit \n",
    "\n",
    "http://localhost:6006/#scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346ce24",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0d8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pen = sns.load_dataset('penguins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b53d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pen = df_pen.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2fda38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pen = df_pen.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beb55cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_pen.drop(['species'], axis = 1)\n",
    "y = df_pen['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8c4bf",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75f34772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_x = ['species', 'island', 'sex']\n",
    "# numerical_x = X.drop(categorical_x, axis = 1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79b920a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## If y is categorical:\n",
    "# # y.fillna(y.mode(), inplace= True)\n",
    "# ##If y is numerical\n",
    "# y.fillna(y.mean(), inplace= True)\n",
    "# for i in numerical_x:\n",
    "#     X[i].fillna(X[i].mean(), inplace = True)\n",
    "\n",
    "# for i in categorical_x:\n",
    "#     X[i].fillna(X[i].mode().iloc[0], inplace = True)\n",
    "    \n",
    "# categoricas = pd.get_dummies(X[categorical_x], drop_first=True)\n",
    "# X = pd.concat([categoricas, X[numerical_x]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a653c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, drop_first = True)\n",
    "y = pd.get_dummies(y, drop_first = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a4a6e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>island_Dream</th>\n",
       "      <th>island_Torgersen</th>\n",
       "      <th>sex_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3950.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.3</td>\n",
       "      <td>13.7</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>185.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "0            38.8           20.0              190.0       3950.0   \n",
       "1            45.3           13.7              210.0       4300.0   \n",
       "2            39.0           18.7              185.0       3650.0   \n",
       "\n",
       "   island_Dream  island_Torgersen  sex_Male  \n",
       "0             1                 0         1  \n",
       "1             0                 0         0  \n",
       "2             1                 0         1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8176b841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adelie</th>\n",
       "      <th>Chinstrap</th>\n",
       "      <th>Gentoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Adelie  Chinstrap  Gentoo\n",
       "0       1          0       0\n",
       "1       0          0       1\n",
       "2       1          0       0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18a55f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50bc7906",
   "metadata": {},
   "outputs": [],
   "source": [
    "escalador = StandardScaler()\n",
    "escalador.fit(X_train)\n",
    "\n",
    "X_train = escalador.transform(X_train)\n",
    "X_test = escalador.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8500d4",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56fd105c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cea72733",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "\n",
    "model.add(Dense(units=8,activation='relu'))\n",
    "\n",
    "model.add(Dense(units=4,activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(units=3,activation='softmax'))\n",
    "\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e6e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.0142 - accuracy: 0.5751 - val_loss: 0.9917 - val_accuracy: 0.6200\n",
      "Epoch 2/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.9874 - accuracy: 0.6052 - val_loss: 0.9692 - val_accuracy: 0.6700\n",
      "Epoch 3/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.9640 - accuracy: 0.6524 - val_loss: 0.9478 - val_accuracy: 0.7300\n",
      "Epoch 4/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.9403 - accuracy: 0.6867 - val_loss: 0.9264 - val_accuracy: 0.7300\n",
      "Epoch 5/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.9176 - accuracy: 0.7253 - val_loss: 0.9044 - val_accuracy: 0.7300\n",
      "Epoch 6/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.8944 - accuracy: 0.7382 - val_loss: 0.8816 - val_accuracy: 0.7500\n",
      "Epoch 7/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.8708 - accuracy: 0.7468 - val_loss: 0.8568 - val_accuracy: 0.7500\n",
      "Epoch 8/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.8465 - accuracy: 0.7511 - val_loss: 0.8296 - val_accuracy: 0.7600\n",
      "Epoch 9/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.8198 - accuracy: 0.7639 - val_loss: 0.8016 - val_accuracy: 0.7600\n",
      "Epoch 10/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.7923 - accuracy: 0.7639 - val_loss: 0.7715 - val_accuracy: 0.7600\n",
      "Epoch 11/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.7625 - accuracy: 0.7725 - val_loss: 0.7397 - val_accuracy: 0.7600\n",
      "Epoch 12/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.7320 - accuracy: 0.7768 - val_loss: 0.7060 - val_accuracy: 0.7700\n",
      "Epoch 13/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.7000 - accuracy: 0.7768 - val_loss: 0.6727 - val_accuracy: 0.7700\n",
      "Epoch 14/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6677 - accuracy: 0.7811 - val_loss: 0.6412 - val_accuracy: 0.7700\n",
      "Epoch 15/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6374 - accuracy: 0.7811 - val_loss: 0.6098 - val_accuracy: 0.7700\n",
      "Epoch 16/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6076 - accuracy: 0.7811 - val_loss: 0.5822 - val_accuracy: 0.7700\n",
      "Epoch 17/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5821 - accuracy: 0.7811 - val_loss: 0.5574 - val_accuracy: 0.7800\n",
      "Epoch 18/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5590 - accuracy: 0.7854 - val_loss: 0.5350 - val_accuracy: 0.7800\n",
      "Epoch 19/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5379 - accuracy: 0.7854 - val_loss: 0.5149 - val_accuracy: 0.7800\n",
      "Epoch 20/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5186 - accuracy: 0.7854 - val_loss: 0.4964 - val_accuracy: 0.7800\n",
      "Epoch 21/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5005 - accuracy: 0.7854 - val_loss: 0.4792 - val_accuracy: 0.7800\n",
      "Epoch 22/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4836 - accuracy: 0.7897 - val_loss: 0.4633 - val_accuracy: 0.7800\n",
      "Epoch 23/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4686 - accuracy: 0.7897 - val_loss: 0.4481 - val_accuracy: 0.7800\n",
      "Epoch 24/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4540 - accuracy: 0.7897 - val_loss: 0.4347 - val_accuracy: 0.7800\n",
      "Epoch 25/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4409 - accuracy: 0.7897 - val_loss: 0.4229 - val_accuracy: 0.7800\n",
      "Epoch 26/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4288 - accuracy: 0.7897 - val_loss: 0.4118 - val_accuracy: 0.7800\n",
      "Epoch 27/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4177 - accuracy: 0.7897 - val_loss: 0.4017 - val_accuracy: 0.7800\n",
      "Epoch 28/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4074 - accuracy: 0.7940 - val_loss: 0.3927 - val_accuracy: 0.7800\n",
      "Epoch 29/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3975 - accuracy: 0.7940 - val_loss: 0.3835 - val_accuracy: 0.7800\n",
      "Epoch 30/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3882 - accuracy: 0.7940 - val_loss: 0.3757 - val_accuracy: 0.7800\n",
      "Epoch 31/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3800 - accuracy: 0.7940 - val_loss: 0.3675 - val_accuracy: 0.7800\n",
      "Epoch 32/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3718 - accuracy: 0.7940 - val_loss: 0.3593 - val_accuracy: 0.7800\n",
      "Epoch 33/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3639 - accuracy: 0.7940 - val_loss: 0.3521 - val_accuracy: 0.7800\n",
      "Epoch 34/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3566 - accuracy: 0.7940 - val_loss: 0.3452 - val_accuracy: 0.7800\n",
      "Epoch 35/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3496 - accuracy: 0.7940 - val_loss: 0.3383 - val_accuracy: 0.7800\n",
      "Epoch 36/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3427 - accuracy: 0.7940 - val_loss: 0.3333 - val_accuracy: 0.7800\n",
      "Epoch 37/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3359 - accuracy: 0.7940 - val_loss: 0.3271 - val_accuracy: 0.7800\n",
      "Epoch 38/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3292 - accuracy: 0.7940 - val_loss: 0.3207 - val_accuracy: 0.7800\n",
      "Epoch 39/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3231 - accuracy: 0.7940 - val_loss: 0.3148 - val_accuracy: 0.7800\n",
      "Epoch 40/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3165 - accuracy: 0.7940 - val_loss: 0.3100 - val_accuracy: 0.7800\n",
      "Epoch 41/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3104 - accuracy: 0.7940 - val_loss: 0.3042 - val_accuracy: 0.8300\n",
      "Epoch 42/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3047 - accuracy: 0.8627 - val_loss: 0.2988 - val_accuracy: 0.9400\n",
      "Epoch 43/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2995 - accuracy: 0.9142 - val_loss: 0.2946 - val_accuracy: 0.9500\n",
      "Epoch 44/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2937 - accuracy: 0.9142 - val_loss: 0.2893 - val_accuracy: 0.9500\n",
      "Epoch 45/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2887 - accuracy: 0.9142 - val_loss: 0.2833 - val_accuracy: 0.9500\n",
      "Epoch 46/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2832 - accuracy: 0.9185 - val_loss: 0.2779 - val_accuracy: 0.9500\n",
      "Epoch 47/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2784 - accuracy: 0.9227 - val_loss: 0.2725 - val_accuracy: 0.9600\n",
      "Epoch 48/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2737 - accuracy: 0.9270 - val_loss: 0.2671 - val_accuracy: 0.9600\n",
      "Epoch 49/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2691 - accuracy: 0.9270 - val_loss: 0.2622 - val_accuracy: 0.9600\n",
      "Epoch 50/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2647 - accuracy: 0.9313 - val_loss: 0.2576 - val_accuracy: 0.9700\n",
      "Epoch 51/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2604 - accuracy: 0.9313 - val_loss: 0.2533 - val_accuracy: 0.9700\n",
      "Epoch 52/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2559 - accuracy: 0.9356 - val_loss: 0.2492 - val_accuracy: 0.9700\n",
      "Epoch 53/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2519 - accuracy: 0.9399 - val_loss: 0.2452 - val_accuracy: 0.9800\n",
      "Epoch 54/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2478 - accuracy: 0.9399 - val_loss: 0.2416 - val_accuracy: 0.9800\n",
      "Epoch 55/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2431 - accuracy: 0.9442 - val_loss: 0.2377 - val_accuracy: 0.9900\n",
      "Epoch 56/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2392 - accuracy: 0.9485 - val_loss: 0.2335 - val_accuracy: 0.9900\n",
      "Epoch 57/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2354 - accuracy: 0.9528 - val_loss: 0.2299 - val_accuracy: 1.0000\n",
      "Epoch 58/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2317 - accuracy: 0.9657 - val_loss: 0.2267 - val_accuracy: 1.0000\n",
      "Epoch 59/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2282 - accuracy: 0.9700 - val_loss: 0.2234 - val_accuracy: 1.0000\n",
      "Epoch 60/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2250 - accuracy: 0.9700 - val_loss: 0.2209 - val_accuracy: 1.0000\n",
      "Epoch 61/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2216 - accuracy: 0.9700 - val_loss: 0.2178 - val_accuracy: 1.0000\n",
      "Epoch 62/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2185 - accuracy: 0.9700 - val_loss: 0.2146 - val_accuracy: 1.0000\n",
      "Epoch 63/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2155 - accuracy: 0.9700 - val_loss: 0.2114 - val_accuracy: 1.0000\n",
      "Epoch 64/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2125 - accuracy: 0.9700 - val_loss: 0.2087 - val_accuracy: 1.0000\n",
      "Epoch 65/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2098 - accuracy: 0.9700 - val_loss: 0.2060 - val_accuracy: 1.0000\n",
      "Epoch 66/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2071 - accuracy: 0.9700 - val_loss: 0.2032 - val_accuracy: 1.0000\n",
      "Epoch 67/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2045 - accuracy: 0.9700 - val_loss: 0.2008 - val_accuracy: 1.0000\n",
      "Epoch 68/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2018 - accuracy: 0.9700 - val_loss: 0.1979 - val_accuracy: 1.0000\n",
      "Epoch 69/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1993 - accuracy: 0.9742 - val_loss: 0.1955 - val_accuracy: 1.0000\n",
      "Epoch 70/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1968 - accuracy: 0.9742 - val_loss: 0.1934 - val_accuracy: 1.0000\n",
      "Epoch 71/600\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1942 - accuracy: 0.9828 - val_loss: 0.1911 - val_accuracy: 1.0000\n",
      "Epoch 72/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1918 - accuracy: 0.9828 - val_loss: 0.1889 - val_accuracy: 1.0000\n",
      "Epoch 73/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1895 - accuracy: 0.9828 - val_loss: 0.1870 - val_accuracy: 1.0000\n",
      "Epoch 74/600\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1872 - accuracy: 0.9871 - val_loss: 0.1846 - val_accuracy: 1.0000\n",
      "Epoch 75/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1850 - accuracy: 0.9871 - val_loss: 0.1823 - val_accuracy: 1.0000\n",
      "Epoch 76/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1828 - accuracy: 0.9871 - val_loss: 0.1802 - val_accuracy: 1.0000\n",
      "Epoch 77/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1807 - accuracy: 0.9871 - val_loss: 0.1783 - val_accuracy: 1.0000\n",
      "Epoch 78/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1787 - accuracy: 0.9871 - val_loss: 0.1763 - val_accuracy: 1.0000\n",
      "Epoch 79/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1769 - accuracy: 0.9871 - val_loss: 0.1751 - val_accuracy: 1.0000\n",
      "Epoch 80/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1746 - accuracy: 0.9871 - val_loss: 0.1733 - val_accuracy: 1.0000\n",
      "Epoch 81/600\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1726 - accuracy: 0.9914 - val_loss: 0.1712 - val_accuracy: 1.0000\n",
      "Epoch 82/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1706 - accuracy: 0.9914 - val_loss: 0.1691 - val_accuracy: 1.0000\n",
      "Epoch 83/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1687 - accuracy: 0.9914 - val_loss: 0.1670 - val_accuracy: 1.0000\n",
      "Epoch 84/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1668 - accuracy: 0.9914 - val_loss: 0.1651 - val_accuracy: 1.0000\n",
      "Epoch 85/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1649 - accuracy: 0.9914 - val_loss: 0.1635 - val_accuracy: 1.0000\n",
      "Epoch 86/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1631 - accuracy: 0.9914 - val_loss: 0.1620 - val_accuracy: 1.0000\n",
      "Epoch 87/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1612 - accuracy: 0.9914 - val_loss: 0.1602 - val_accuracy: 1.0000\n",
      "Epoch 88/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1595 - accuracy: 0.9914 - val_loss: 0.1582 - val_accuracy: 1.0000\n",
      "Epoch 89/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1578 - accuracy: 0.9914 - val_loss: 0.1564 - val_accuracy: 1.0000\n",
      "Epoch 90/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1561 - accuracy: 0.9914 - val_loss: 0.1548 - val_accuracy: 1.0000\n",
      "Epoch 91/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1545 - accuracy: 0.9914 - val_loss: 0.1534 - val_accuracy: 1.0000\n",
      "Epoch 92/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1530 - accuracy: 0.9914 - val_loss: 0.1516 - val_accuracy: 1.0000\n",
      "Epoch 93/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1514 - accuracy: 0.9914 - val_loss: 0.1500 - val_accuracy: 1.0000\n",
      "Epoch 94/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1499 - accuracy: 0.9914 - val_loss: 0.1485 - val_accuracy: 1.0000\n",
      "Epoch 95/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1484 - accuracy: 0.9914 - val_loss: 0.1467 - val_accuracy: 1.0000\n",
      "Epoch 96/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1468 - accuracy: 0.9914 - val_loss: 0.1452 - val_accuracy: 1.0000\n",
      "Epoch 97/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1455 - accuracy: 0.9914 - val_loss: 0.1437 - val_accuracy: 1.0000\n",
      "Epoch 98/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1440 - accuracy: 0.9914 - val_loss: 0.1422 - val_accuracy: 1.0000\n",
      "Epoch 99/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1427 - accuracy: 0.9914 - val_loss: 0.1408 - val_accuracy: 1.0000\n",
      "Epoch 100/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1410 - accuracy: 0.9914 - val_loss: 0.1394 - val_accuracy: 1.0000\n",
      "Epoch 101/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1397 - accuracy: 0.9914 - val_loss: 0.1379 - val_accuracy: 1.0000\n",
      "Epoch 102/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1384 - accuracy: 0.9914 - val_loss: 0.1364 - val_accuracy: 1.0000\n",
      "Epoch 103/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1370 - accuracy: 0.9914 - val_loss: 0.1351 - val_accuracy: 1.0000\n",
      "Epoch 104/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1355 - accuracy: 0.9914 - val_loss: 0.1338 - val_accuracy: 1.0000\n",
      "Epoch 105/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1341 - accuracy: 0.9914 - val_loss: 0.1325 - val_accuracy: 1.0000\n",
      "Epoch 106/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1329 - accuracy: 0.9914 - val_loss: 0.1309 - val_accuracy: 1.0000\n",
      "Epoch 107/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1313 - accuracy: 0.9914 - val_loss: 0.1300 - val_accuracy: 1.0000\n",
      "Epoch 108/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1301 - accuracy: 0.9957 - val_loss: 0.1287 - val_accuracy: 1.0000\n",
      "Epoch 109/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1289 - accuracy: 0.9957 - val_loss: 0.1276 - val_accuracy: 1.0000\n",
      "Epoch 110/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1276 - accuracy: 0.9957 - val_loss: 0.1260 - val_accuracy: 1.0000\n",
      "Epoch 111/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1263 - accuracy: 0.9957 - val_loss: 0.1245 - val_accuracy: 1.0000\n",
      "Epoch 112/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1251 - accuracy: 0.9914 - val_loss: 0.1233 - val_accuracy: 1.0000\n",
      "Epoch 113/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1241 - accuracy: 0.9914 - val_loss: 0.1219 - val_accuracy: 1.0000\n",
      "Epoch 114/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1229 - accuracy: 0.9914 - val_loss: 0.1209 - val_accuracy: 1.0000\n",
      "Epoch 115/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1216 - accuracy: 0.9957 - val_loss: 0.1198 - val_accuracy: 1.0000\n",
      "Epoch 116/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1205 - accuracy: 0.9957 - val_loss: 0.1187 - val_accuracy: 1.0000\n",
      "Epoch 117/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1194 - accuracy: 0.9957 - val_loss: 0.1177 - val_accuracy: 1.0000\n",
      "Epoch 118/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1183 - accuracy: 0.9957 - val_loss: 0.1163 - val_accuracy: 1.0000\n",
      "Epoch 119/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1173 - accuracy: 0.9914 - val_loss: 0.1152 - val_accuracy: 1.0000\n",
      "Epoch 120/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1163 - accuracy: 0.9914 - val_loss: 0.1142 - val_accuracy: 1.0000\n",
      "Epoch 121/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1153 - accuracy: 0.9957 - val_loss: 0.1133 - val_accuracy: 1.0000\n",
      "Epoch 122/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1141 - accuracy: 0.9957 - val_loss: 0.1123 - val_accuracy: 1.0000\n",
      "Epoch 123/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1131 - accuracy: 0.9957 - val_loss: 0.1113 - val_accuracy: 1.0000\n",
      "Epoch 124/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1122 - accuracy: 0.9957 - val_loss: 0.1104 - val_accuracy: 1.0000\n",
      "Epoch 125/600\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.1113 - accuracy: 0.9957 - val_loss: 0.1095 - val_accuracy: 1.0000\n",
      "Epoch 126/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1102 - accuracy: 0.9957 - val_loss: 0.1085 - val_accuracy: 1.0000\n",
      "Epoch 127/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1093 - accuracy: 0.9957 - val_loss: 0.1075 - val_accuracy: 1.0000\n",
      "Epoch 128/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1084 - accuracy: 0.9957 - val_loss: 0.1065 - val_accuracy: 1.0000\n",
      "Epoch 129/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1075 - accuracy: 0.9957 - val_loss: 0.1060 - val_accuracy: 1.0000\n",
      "Epoch 130/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1064 - accuracy: 1.0000 - val_loss: 0.1050 - val_accuracy: 1.0000\n",
      "Epoch 131/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1054 - accuracy: 1.0000 - val_loss: 0.1040 - val_accuracy: 1.0000\n",
      "Epoch 132/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1045 - accuracy: 1.0000 - val_loss: 0.1029 - val_accuracy: 1.0000\n",
      "Epoch 133/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1035 - accuracy: 1.0000 - val_loss: 0.1019 - val_accuracy: 1.0000\n",
      "Epoch 134/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1027 - accuracy: 1.0000 - val_loss: 0.1012 - val_accuracy: 1.0000\n",
      "Epoch 135/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1017 - accuracy: 1.0000 - val_loss: 0.1002 - val_accuracy: 1.0000\n",
      "Epoch 136/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1007 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 1.0000\n",
      "Epoch 137/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0998 - accuracy: 1.0000 - val_loss: 0.0982 - val_accuracy: 1.0000\n",
      "Epoch 138/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0990 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 1.0000\n",
      "Epoch 139/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0981 - accuracy: 1.0000 - val_loss: 0.0967 - val_accuracy: 1.0000\n",
      "Epoch 140/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0974 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 1.0000\n",
      "Epoch 141/600\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0966 - accuracy: 1.0000 - val_loss: 0.0950 - val_accuracy: 1.0000\n",
      "Epoch 142/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0957 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 1.0000\n",
      "Epoch 143/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0948 - accuracy: 1.0000 - val_loss: 0.0932 - val_accuracy: 1.0000\n",
      "Epoch 144/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0939 - accuracy: 1.0000 - val_loss: 0.0922 - val_accuracy: 1.0000\n",
      "Epoch 145/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0932 - accuracy: 1.0000 - val_loss: 0.0913 - val_accuracy: 1.0000\n",
      "Epoch 146/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0924 - accuracy: 1.0000 - val_loss: 0.0905 - val_accuracy: 1.0000\n",
      "Epoch 147/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0916 - accuracy: 1.0000 - val_loss: 0.0897 - val_accuracy: 1.0000\n",
      "Epoch 148/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0908 - accuracy: 1.0000 - val_loss: 0.0890 - val_accuracy: 1.0000\n",
      "Epoch 149/600\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0900 - accuracy: 1.0000 - val_loss: 0.0883 - val_accuracy: 1.0000\n",
      "Epoch 150/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0893 - accuracy: 1.0000 - val_loss: 0.0876 - val_accuracy: 1.0000\n",
      "Epoch 151/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0887 - accuracy: 1.0000 - val_loss: 0.0870 - val_accuracy: 1.0000\n",
      "Epoch 152/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0879 - accuracy: 1.0000 - val_loss: 0.0863 - val_accuracy: 1.0000\n",
      "Epoch 153/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0872 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 1.0000\n",
      "Epoch 154/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0866 - accuracy: 1.0000 - val_loss: 0.0847 - val_accuracy: 1.0000\n",
      "Epoch 155/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0858 - accuracy: 1.0000 - val_loss: 0.0840 - val_accuracy: 1.0000\n",
      "Epoch 156/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0851 - accuracy: 1.0000 - val_loss: 0.0832 - val_accuracy: 1.0000\n",
      "Epoch 157/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0844 - accuracy: 1.0000 - val_loss: 0.0826 - val_accuracy: 1.0000\n",
      "Epoch 158/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0837 - accuracy: 1.0000 - val_loss: 0.0819 - val_accuracy: 1.0000\n",
      "Epoch 159/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0830 - accuracy: 1.0000 - val_loss: 0.0811 - val_accuracy: 1.0000\n",
      "Epoch 160/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0823 - accuracy: 1.0000 - val_loss: 0.0804 - val_accuracy: 1.0000\n",
      "Epoch 161/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0816 - accuracy: 1.0000 - val_loss: 0.0797 - val_accuracy: 1.0000\n",
      "Epoch 162/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0810 - accuracy: 1.0000 - val_loss: 0.0791 - val_accuracy: 1.0000\n",
      "Epoch 163/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0804 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 1.0000\n",
      "Epoch 164/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0798 - accuracy: 1.0000 - val_loss: 0.0780 - val_accuracy: 1.0000\n",
      "Epoch 165/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0791 - accuracy: 1.0000 - val_loss: 0.0773 - val_accuracy: 1.0000\n",
      "Epoch 166/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0785 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 1.0000\n",
      "Epoch 167/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0778 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 1.0000\n",
      "Epoch 168/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0773 - accuracy: 1.0000 - val_loss: 0.0754 - val_accuracy: 1.0000\n",
      "Epoch 169/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0766 - accuracy: 1.0000 - val_loss: 0.0747 - val_accuracy: 1.0000\n",
      "Epoch 170/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0760 - accuracy: 1.0000 - val_loss: 0.0741 - val_accuracy: 1.0000\n",
      "Epoch 171/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0754 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 1.0000\n",
      "Epoch 172/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0748 - accuracy: 1.0000 - val_loss: 0.0729 - val_accuracy: 1.0000\n",
      "Epoch 173/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0742 - accuracy: 1.0000 - val_loss: 0.0723 - val_accuracy: 1.0000\n",
      "Epoch 174/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0737 - accuracy: 1.0000 - val_loss: 0.0717 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0730 - accuracy: 1.0000 - val_loss: 0.0712 - val_accuracy: 1.0000\n",
      "Epoch 176/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0725 - accuracy: 1.0000 - val_loss: 0.0706 - val_accuracy: 1.0000\n",
      "Epoch 177/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0719 - accuracy: 1.0000 - val_loss: 0.0700 - val_accuracy: 1.0000\n",
      "Epoch 178/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0713 - accuracy: 1.0000 - val_loss: 0.0694 - val_accuracy: 1.0000\n",
      "Epoch 179/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0708 - accuracy: 1.0000 - val_loss: 0.0689 - val_accuracy: 1.0000\n",
      "Epoch 180/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0702 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 1.0000\n",
      "Epoch 181/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0697 - accuracy: 1.0000 - val_loss: 0.0678 - val_accuracy: 1.0000\n",
      "Epoch 182/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0695 - accuracy: 1.0000 - val_loss: 0.0672 - val_accuracy: 1.0000\n",
      "Epoch 183/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0687 - accuracy: 1.0000 - val_loss: 0.0667 - val_accuracy: 1.0000\n",
      "Epoch 184/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0681 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
      "Epoch 185/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0676 - accuracy: 1.0000 - val_loss: 0.0657 - val_accuracy: 1.0000\n",
      "Epoch 186/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0671 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 1.0000\n",
      "Epoch 187/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0665 - accuracy: 1.0000 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
      "Epoch 188/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0659 - accuracy: 1.0000 - val_loss: 0.0641 - val_accuracy: 1.0000\n",
      "Epoch 189/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0656 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 1.0000\n",
      "Epoch 190/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0651 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 1.0000\n",
      "Epoch 191/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0645 - accuracy: 1.0000 - val_loss: 0.0626 - val_accuracy: 1.0000\n",
      "Epoch 192/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 1.0000\n",
      "Epoch 193/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 0.0617 - val_accuracy: 1.0000\n",
      "Epoch 194/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0631 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 1.0000\n",
      "Epoch 195/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0626 - accuracy: 1.0000 - val_loss: 0.0607 - val_accuracy: 1.0000\n",
      "Epoch 196/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0622 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 1.0000\n",
      "Epoch 197/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0617 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 1.0000\n",
      "Epoch 198/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0612 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 1.0000\n",
      "Epoch 199/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0608 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 1.0000\n",
      "Epoch 200/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 0.0585 - val_accuracy: 1.0000\n",
      "Epoch 201/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0600 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 1.0000\n",
      "Epoch 202/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0595 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 1.0000\n",
      "Epoch 203/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0591 - accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 1.0000\n",
      "Epoch 204/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0589 - accuracy: 1.0000 - val_loss: 0.0567 - val_accuracy: 1.0000\n",
      "Epoch 205/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0583 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 1.0000\n",
      "Epoch 206/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0578 - accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 1.0000\n",
      "Epoch 207/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0574 - accuracy: 1.0000 - val_loss: 0.0555 - val_accuracy: 1.0000\n",
      "Epoch 208/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0569 - accuracy: 1.0000 - val_loss: 0.0552 - val_accuracy: 1.0000\n",
      "Epoch 209/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0567 - accuracy: 1.0000 - val_loss: 0.0548 - val_accuracy: 1.0000\n",
      "Epoch 210/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 1.0000\n",
      "Epoch 211/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0560 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 1.0000\n",
      "Epoch 212/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0554 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 1.0000\n",
      "Epoch 213/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0550 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 1.0000\n",
      "Epoch 214/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 1.0000\n",
      "Epoch 215/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0541 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 1.0000\n",
      "Epoch 216/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0537 - accuracy: 1.0000 - val_loss: 0.0519 - val_accuracy: 1.0000\n",
      "Epoch 217/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0533 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 1.0000\n",
      "Epoch 218/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 1.0000\n",
      "Epoch 219/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 1.0000\n",
      "Epoch 220/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0522 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 1.0000\n",
      "Epoch 221/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0519 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 1.0000\n",
      "Epoch 222/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 0.0497 - val_accuracy: 1.0000\n",
      "Epoch 223/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0511 - accuracy: 1.0000 - val_loss: 0.0493 - val_accuracy: 1.0000\n",
      "Epoch 224/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
      "Epoch 225/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0504 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 1.0000\n",
      "Epoch 226/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0500 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 1.0000\n",
      "Epoch 227/600\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0496 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 1.0000\n",
      "Epoch 228/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0493 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 1.0000\n",
      "Epoch 229/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0489 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 1.0000\n",
      "Epoch 230/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.0468 - val_accuracy: 1.0000\n",
      "Epoch 231/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0483 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
      "Epoch 232/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0479 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0476 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 1.0000\n",
      "Epoch 234/600\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0473 - accuracy: 1.0000 - val_loss: 0.0455 - val_accuracy: 1.0000\n",
      "Epoch 235/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0470 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
      "Epoch 236/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.0449 - val_accuracy: 1.0000\n",
      "Epoch 237/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0463 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
      "Epoch 238/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 1.0000\n",
      "Epoch 239/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 0.0439 - val_accuracy: 1.0000\n",
      "Epoch 240/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0454 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 1.0000\n",
      "Epoch 241/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 1.0000\n",
      "Epoch 242/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 0.0431 - val_accuracy: 1.0000\n",
      "Epoch 243/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
      "Epoch 244/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 0.0425 - val_accuracy: 1.0000\n",
      "Epoch 245/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0439 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
      "Epoch 246/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.0419 - val_accuracy: 1.0000\n",
      "Epoch 247/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
      "Epoch 248/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0431 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 1.0000\n",
      "Epoch 249/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0428 - accuracy: 1.0000 - val_loss: 0.0411 - val_accuracy: 1.0000\n",
      "Epoch 250/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.0408 - val_accuracy: 1.0000\n",
      "Epoch 251/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.0405 - val_accuracy: 1.0000\n",
      "Epoch 252/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 1.0000\n",
      "Epoch 253/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0416 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
      "Epoch 254/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0413 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
      "Epoch 255/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 1.0000\n",
      "Epoch 256/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
      "Epoch 257/600\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0406 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
      "Epoch 258/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
      "Epoch 259/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0401 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
      "Epoch 260/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 261/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 262/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0392 - accuracy: 1.0000 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
      "Epoch 263/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 264/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
      "Epoch 265/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
      "Epoch 266/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 1.0000\n",
      "Epoch 267/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
      "Epoch 268/600\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0377 - accuracy: 1.0000 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
      "Epoch 269/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.0359 - val_accuracy: 1.0000\n",
      "Epoch 270/600\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
      "Epoch 271/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
      "Epoch 272/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 0.0352 - val_accuracy: 1.0000\n",
      "Epoch 273/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.0350 - val_accuracy: 1.0000\n",
      "Epoch 274/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
      "Epoch 275/600\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
      "Epoch 276/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0359 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 1.0000\n",
      "Epoch 277/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
      "Epoch 278/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
      "Epoch 279/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
      "Epoch 280/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
      "Epoch 281/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 1.0000\n",
      "Epoch 282/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
      "Epoch 283/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.0328 - val_accuracy: 1.0000\n",
      "Epoch 284/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 1.0000\n",
      "Epoch 285/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
      "Epoch 286/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
      "Epoch 287/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
      "Epoch 288/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
      "Epoch 289/600\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
      "Epoch 290/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0328 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
      "Epoch 292/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
      "Epoch 293/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.0308 - val_accuracy: 1.0000\n",
      "Epoch 294/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
      "Epoch 295/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 1.0000\n",
      "Epoch 296/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0303 - val_accuracy: 1.0000\n",
      "Epoch 297/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 1.0000\n",
      "Epoch 298/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 299/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
      "Epoch 300/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.0295 - val_accuracy: 1.0000\n",
      "Epoch 301/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
      "Epoch 302/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 303/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
      "Epoch 304/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 305/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0286 - val_accuracy: 1.0000\n",
      "Epoch 306/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.0284 - val_accuracy: 1.0000\n",
      "Epoch 307/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
      "Epoch 308/600\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 309/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0279 - val_accuracy: 1.0000\n",
      "Epoch 310/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
      "Epoch 311/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 312/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 313/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
      "Epoch 314/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 315/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 316/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
      "Epoch 317/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 318/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 319/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 320/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 321/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 322/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 323/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
      "Epoch 324/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 325/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 326/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 327/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 328/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 329/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 330/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0258 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 331/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 332/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 333/600\n",
      "8/8 [==============================] - 1s 213ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 334/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 1.0000\n",
      "Epoch 335/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
      "Epoch 336/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 1.0000\n",
      "Epoch 337/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 1.0000\n",
      "Epoch 338/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
      "Epoch 339/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
      "Epoch 340/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 341/600\n",
      "8/8 [==============================] - 2s 291ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
      "Epoch 342/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 1.0000\n",
      "Epoch 343/600\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
      "Epoch 344/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 1.0000\n",
      "Epoch 345/600\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
      "Epoch 346/600\n",
      "8/8 [==============================] - 2s 294ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
      "Epoch 347/600\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
      "Epoch 348/600\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/600\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 1.0000\n",
      "Epoch 350/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 1.0000\n",
      "Epoch 351/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
      "Epoch 352/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
      "Epoch 353/600\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 1.0000\n",
      "Epoch 354/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
      "Epoch 355/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
      "Epoch 356/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
      "Epoch 357/600\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
      "Epoch 358/600\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
      "Epoch 359/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.0210 - val_accuracy: 1.0000\n",
      "Epoch 360/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 1.0000\n",
      "Epoch 361/600\n",
      "8/8 [==============================] - 2s 305ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
      "Epoch 362/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.0206 - val_accuracy: 1.0000\n",
      "Epoch 363/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
      "Epoch 364/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 1.0000\n",
      "Epoch 365/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.0203 - val_accuracy: 1.0000\n",
      "Epoch 366/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.0202 - val_accuracy: 1.0000\n",
      "Epoch 367/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000\n",
      "Epoch 368/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
      "Epoch 369/600\n",
      "8/8 [==============================] - 2s 267ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
      "Epoch 370/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 1.0000\n",
      "Epoch 371/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
      "Epoch 372/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
      "Epoch 373/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.0194 - val_accuracy: 1.0000\n",
      "Epoch 374/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
      "Epoch 375/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
      "Epoch 376/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 1.0000\n",
      "Epoch 377/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
      "Epoch 378/600\n",
      "8/8 [==============================] - 2s 256ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.0189 - val_accuracy: 1.0000\n",
      "Epoch 379/600\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
      "Epoch 380/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.0187 - val_accuracy: 1.0000\n",
      "Epoch 381/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 1.0000\n",
      "Epoch 382/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
      "Epoch 383/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
      "Epoch 384/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
      "Epoch 385/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 1.0000\n",
      "Epoch 386/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 1.0000\n",
      "Epoch 387/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.0180 - val_accuracy: 1.0000\n",
      "Epoch 388/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 1.0000\n",
      "Epoch 389/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
      "Epoch 390/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
      "Epoch 391/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.0176 - val_accuracy: 1.0000\n",
      "Epoch 392/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
      "Epoch 393/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
      "Epoch 394/600\n",
      "8/8 [==============================] - 6s 822ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
      "Epoch 395/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
      "Epoch 396/600\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.0172 - val_accuracy: 1.0000\n",
      "Epoch 397/600\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
      "Epoch 398/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
      "Epoch 399/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0169 - val_accuracy: 1.0000\n",
      "Epoch 400/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
      "Epoch 401/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
      "Epoch 402/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0166 - val_accuracy: 1.0000\n",
      "Epoch 403/600\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0166 - val_accuracy: 1.0000\n",
      "Epoch 404/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
      "Epoch 405/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.0164 - val_accuracy: 1.0000\n",
      "Epoch 406/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.0162 - val_accuracy: 1.0000\n",
      "Epoch 408/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
      "Epoch 409/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
      "Epoch 410/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
      "Epoch 411/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
      "Epoch 412/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
      "Epoch 413/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
      "Epoch 414/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
      "Epoch 415/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
      "Epoch 416/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
      "Epoch 417/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
      "Epoch 418/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
      "Epoch 419/600\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
      "Epoch 420/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
      "Epoch 421/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
      "Epoch 422/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
      "Epoch 423/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
      "Epoch 424/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
      "Epoch 425/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 426/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 427/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
      "Epoch 428/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
      "Epoch 429/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
      "Epoch 430/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
      "Epoch 431/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 432/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
      "Epoch 433/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
      "Epoch 434/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
      "Epoch 435/600\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
      "Epoch 436/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
      "Epoch 437/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
      "Epoch 438/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
      "Epoch 439/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
      "Epoch 440/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
      "Epoch 441/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 442/600\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
      "Epoch 443/600\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
      "Epoch 444/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
      "Epoch 445/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
      "Epoch 446/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
      "Epoch 447/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
      "Epoch 448/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
      "Epoch 449/600\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
      "Epoch 450/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "Epoch 451/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
      "Epoch 452/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
      "Epoch 453/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
      "Epoch 454/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
      "Epoch 455/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
      "Epoch 456/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
      "Epoch 457/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 458/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 459/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
      "Epoch 460/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
      "Epoch 461/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 462/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 463/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
      "Epoch 464/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
      "Epoch 466/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 467/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 468/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
      "Epoch 469/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
      "Epoch 470/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
      "Epoch 471/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 472/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 473/600\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
      "Epoch 474/600\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
      "Epoch 475/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
      "Epoch 476/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
      "Epoch 477/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
      "Epoch 478/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
      "Epoch 479/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
      "Epoch 480/600\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 481/600\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 482/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
      "Epoch 483/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
      "Epoch 484/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
      "Epoch 485/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
      "Epoch 486/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
      "Epoch 487/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
      "Epoch 488/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
      "Epoch 489/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
      "Epoch 490/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 491/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 492/600\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
      "Epoch 493/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
      "Epoch 494/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 495/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 496/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 497/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 498/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
      "Epoch 499/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
      "Epoch 500/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
      "Epoch 501/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
      "Epoch 502/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 503/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 504/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 505/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 506/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 507/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 508/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
      "Epoch 509/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
      "Epoch 510/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
      "Epoch 511/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 512/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 513/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "Epoch 514/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "Epoch 515/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
      "Epoch 516/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
      "Epoch 517/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "Epoch 518/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "Epoch 519/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "Epoch 520/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
      "Epoch 521/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
      "Epoch 522/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
      "Epoch 524/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 525/600\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 526/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 527/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 528/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 529/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
      "Epoch 530/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
      "Epoch 531/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "Epoch 532/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "Epoch 533/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "Epoch 534/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
      "Epoch 535/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
      "Epoch 536/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "Epoch 537/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "Epoch 538/600\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "Epoch 539/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 540/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 541/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 542/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 543/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 544/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
      "Epoch 545/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
      "Epoch 546/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
      "Epoch 547/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 548/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 549/600\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 550/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 551/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 552/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 553/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 554/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 555/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 556/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 557/600\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 558/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 559/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 560/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 561/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 562/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 563/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 564/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 565/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 566/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 567/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 568/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 569/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 570/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 571/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 572/600\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 573/600\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 574/600\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 575/600\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 576/600\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 577/600\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 578/600\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 579/600\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 580/600\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 581/600\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 582/600\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 583/600\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 584/600\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 585/600\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
      "Epoch 586/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
      "Epoch 587/600\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
      "Epoch 588/600\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 589/600\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 590/600\n",
      "8/8 [==============================] - 1s 47ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 591/600\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
      "Epoch 592/600\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
      "Epoch 593/600\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
      "Epoch 594/600\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
      "Epoch 595/600\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 596/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 597/600\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 598/600\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 599/600\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 600/600\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b8238eb00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network\n",
    "# https://datascience.stackexchange.com/questions/18414/are-there-any-rules-for-choosing-the-size-of-a-mini-batch\n",
    "\n",
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d919eb",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6f78f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.014245629310608,\n",
       "  0.9874083399772644,\n",
       "  0.9640406370162964,\n",
       "  0.9402735829353333,\n",
       "  0.9175538420677185,\n",
       "  0.894413411617279,\n",
       "  0.870796263217926,\n",
       "  0.8464851975440979,\n",
       "  0.8197606801986694,\n",
       "  0.7923303842544556,\n",
       "  0.7625499367713928,\n",
       "  0.7319774627685547,\n",
       "  0.6999807357788086,\n",
       "  0.6677291393280029,\n",
       "  0.6374019384384155,\n",
       "  0.6076314449310303,\n",
       "  0.5821143388748169,\n",
       "  0.5590055584907532,\n",
       "  0.5378987193107605,\n",
       "  0.5186278820037842,\n",
       "  0.5004635453224182,\n",
       "  0.4836018979549408,\n",
       "  0.46856245398521423,\n",
       "  0.4539562165737152,\n",
       "  0.4409276247024536,\n",
       "  0.42881831526756287,\n",
       "  0.4176534116268158,\n",
       "  0.40742605924606323,\n",
       "  0.39748454093933105,\n",
       "  0.3882491886615753,\n",
       "  0.3799588978290558,\n",
       "  0.3717937171459198,\n",
       "  0.36388203501701355,\n",
       "  0.3566003143787384,\n",
       "  0.34960606694221497,\n",
       "  0.34271201491355896,\n",
       "  0.3358807861804962,\n",
       "  0.32923784852027893,\n",
       "  0.3231084942817688,\n",
       "  0.3164535462856293,\n",
       "  0.31044626235961914,\n",
       "  0.3046657145023346,\n",
       "  0.29952898621559143,\n",
       "  0.293701171875,\n",
       "  0.2886962890625,\n",
       "  0.28324517607688904,\n",
       "  0.2784403860569,\n",
       "  0.2736511826515198,\n",
       "  0.2691321074962616,\n",
       "  0.26471686363220215,\n",
       "  0.2604333162307739,\n",
       "  0.255913108587265,\n",
       "  0.25191012024879456,\n",
       "  0.24779391288757324,\n",
       "  0.2431126832962036,\n",
       "  0.2392369657754898,\n",
       "  0.23543484508991241,\n",
       "  0.23172852396965027,\n",
       "  0.2281818687915802,\n",
       "  0.22495920956134796,\n",
       "  0.22157850861549377,\n",
       "  0.21849393844604492,\n",
       "  0.2155393362045288,\n",
       "  0.2125438153743744,\n",
       "  0.20975524187088013,\n",
       "  0.2070583999156952,\n",
       "  0.2045086771249771,\n",
       "  0.2018263041973114,\n",
       "  0.19931647181510925,\n",
       "  0.19675621390342712,\n",
       "  0.19416359066963196,\n",
       "  0.1918264925479889,\n",
       "  0.18951740860939026,\n",
       "  0.18723075091838837,\n",
       "  0.18498550355434418,\n",
       "  0.18283431231975555,\n",
       "  0.1807342916727066,\n",
       "  0.17874574661254883,\n",
       "  0.17687679827213287,\n",
       "  0.1745591163635254,\n",
       "  0.17260143160820007,\n",
       "  0.17059054970741272,\n",
       "  0.16872601211071014,\n",
       "  0.1668281853199005,\n",
       "  0.16485893726348877,\n",
       "  0.1631391942501068,\n",
       "  0.1611684262752533,\n",
       "  0.15948505699634552,\n",
       "  0.15777626633644104,\n",
       "  0.15609712898731232,\n",
       "  0.15451863408088684,\n",
       "  0.15300321578979492,\n",
       "  0.15140481293201447,\n",
       "  0.1498912125825882,\n",
       "  0.1484227478504181,\n",
       "  0.14679893851280212,\n",
       "  0.14545245468616486,\n",
       "  0.14395977556705475,\n",
       "  0.14266027510166168,\n",
       "  0.14104461669921875,\n",
       "  0.13968448340892792,\n",
       "  0.13837304711341858,\n",
       "  0.1369723230600357,\n",
       "  0.13551704585552216,\n",
       "  0.13412140309810638,\n",
       "  0.13288770616054535,\n",
       "  0.13130377233028412,\n",
       "  0.13007867336273193,\n",
       "  0.12890779972076416,\n",
       "  0.12764202058315277,\n",
       "  0.12634731829166412,\n",
       "  0.12509894371032715,\n",
       "  0.12409332394599915,\n",
       "  0.12294492870569229,\n",
       "  0.12156893312931061,\n",
       "  0.1204560175538063,\n",
       "  0.11935043334960938,\n",
       "  0.11833445727825165,\n",
       "  0.11733106523752213,\n",
       "  0.11634670943021774,\n",
       "  0.11527730524539948,\n",
       "  0.11410710215568542,\n",
       "  0.11312933266162872,\n",
       "  0.11219406127929688,\n",
       "  0.11127424240112305,\n",
       "  0.11016292870044708,\n",
       "  0.10926569998264313,\n",
       "  0.1084471046924591,\n",
       "  0.10745806246995926,\n",
       "  0.10636279731988907,\n",
       "  0.10540127754211426,\n",
       "  0.10450563579797745,\n",
       "  0.10353032499551773,\n",
       "  0.1027083694934845,\n",
       "  0.10171137005090714,\n",
       "  0.10074487328529358,\n",
       "  0.09982020407915115,\n",
       "  0.09897404909133911,\n",
       "  0.09810230135917664,\n",
       "  0.09736116230487823,\n",
       "  0.09656078368425369,\n",
       "  0.0956965759396553,\n",
       "  0.09475790709257126,\n",
       "  0.09385315328836441,\n",
       "  0.09315422177314758,\n",
       "  0.09235596656799316,\n",
       "  0.09163092076778412,\n",
       "  0.090799480676651,\n",
       "  0.09004015475511551,\n",
       "  0.08928516507148743,\n",
       "  0.08868753910064697,\n",
       "  0.08793077617883682,\n",
       "  0.08721406757831573,\n",
       "  0.08655474334955215,\n",
       "  0.08577324450016022,\n",
       "  0.08505944907665253,\n",
       "  0.084363654255867,\n",
       "  0.08368459343910217,\n",
       "  0.08297693729400635,\n",
       "  0.08228280395269394,\n",
       "  0.08164706081151962,\n",
       "  0.08102356642484665,\n",
       "  0.08044259250164032,\n",
       "  0.0798405185341835,\n",
       "  0.07909398525953293,\n",
       "  0.07848179340362549,\n",
       "  0.07784280925989151,\n",
       "  0.07726151496171951,\n",
       "  0.07655476778745651,\n",
       "  0.07598370313644409,\n",
       "  0.07542499899864197,\n",
       "  0.07480762898921967,\n",
       "  0.07419834285974503,\n",
       "  0.073691226541996,\n",
       "  0.07302287966012955,\n",
       "  0.07253461331129074,\n",
       "  0.07188671082258224,\n",
       "  0.07130961120128632,\n",
       "  0.0707717314362526,\n",
       "  0.07022595405578613,\n",
       "  0.0697161853313446,\n",
       "  0.06947211176156998,\n",
       "  0.06871681660413742,\n",
       "  0.06810661405324936,\n",
       "  0.06757473945617676,\n",
       "  0.06706561148166656,\n",
       "  0.0665096715092659,\n",
       "  0.06592857837677002,\n",
       "  0.0656299740076065,\n",
       "  0.06507454067468643,\n",
       "  0.06450780481100082,\n",
       "  0.0640542283654213,\n",
       "  0.06361541897058487,\n",
       "  0.06312588602304459,\n",
       "  0.0626329630613327,\n",
       "  0.06224345043301582,\n",
       "  0.06167054921388626,\n",
       "  0.061206523329019547,\n",
       "  0.0608014278113842,\n",
       "  0.06036698818206787,\n",
       "  0.05996108427643776,\n",
       "  0.05953945592045784,\n",
       "  0.059058647602796555,\n",
       "  0.058855023235082626,\n",
       "  0.05828085541725159,\n",
       "  0.057825785130262375,\n",
       "  0.05741109699010849,\n",
       "  0.05687933787703514,\n",
       "  0.0567212775349617,\n",
       "  0.05636729672551155,\n",
       "  0.05599786713719368,\n",
       "  0.05538126453757286,\n",
       "  0.055030301213264465,\n",
       "  0.05453350022435188,\n",
       "  0.0541076585650444,\n",
       "  0.0537261888384819,\n",
       "  0.05331642925739288,\n",
       "  0.052990712225437164,\n",
       "  0.05262845754623413,\n",
       "  0.05216782167553902,\n",
       "  0.05187905579805374,\n",
       "  0.051561884582042694,\n",
       "  0.05112742260098457,\n",
       "  0.050753455609083176,\n",
       "  0.05038626119494438,\n",
       "  0.04999083653092384,\n",
       "  0.04962493106722832,\n",
       "  0.04929259791970253,\n",
       "  0.048931196331977844,\n",
       "  0.048588670790195465,\n",
       "  0.04833818972110748,\n",
       "  0.04794527590274811,\n",
       "  0.047611698508262634,\n",
       "  0.04729021340608597,\n",
       "  0.04698382690548897,\n",
       "  0.046664658933877945,\n",
       "  0.04633622244000435,\n",
       "  0.04602813348174095,\n",
       "  0.04572669416666031,\n",
       "  0.04544734209775925,\n",
       "  0.04504268616437912,\n",
       "  0.044808097183704376,\n",
       "  0.04451063647866249,\n",
       "  0.044232118874788284,\n",
       "  0.04394449293613434,\n",
       "  0.0435846745967865,\n",
       "  0.043334122747182846,\n",
       "  0.04308800771832466,\n",
       "  0.0428103469312191,\n",
       "  0.04248709976673126,\n",
       "  0.04220584034919739,\n",
       "  0.041995301842689514,\n",
       "  0.041589293628931046,\n",
       "  0.04134372994303703,\n",
       "  0.0411500409245491,\n",
       "  0.040869612246751785,\n",
       "  0.04059717059135437,\n",
       "  0.04034055396914482,\n",
       "  0.0400526337325573,\n",
       "  0.03971954062581062,\n",
       "  0.03951376676559448,\n",
       "  0.03924700617790222,\n",
       "  0.038991108536720276,\n",
       "  0.03880700096487999,\n",
       "  0.038588374853134155,\n",
       "  0.038202960044145584,\n",
       "  0.0380130372941494,\n",
       "  0.037701237946748734,\n",
       "  0.03748442232608795,\n",
       "  0.037208061665296555,\n",
       "  0.03695954754948616,\n",
       "  0.03675025328993797,\n",
       "  0.03650248795747757,\n",
       "  0.03626122325658798,\n",
       "  0.0360737107694149,\n",
       "  0.03590112179517746,\n",
       "  0.03561367467045784,\n",
       "  0.03544343635439873,\n",
       "  0.03513336554169655,\n",
       "  0.03488102927803993,\n",
       "  0.03467605635523796,\n",
       "  0.03447245433926582,\n",
       "  0.03423568606376648,\n",
       "  0.034030232578516006,\n",
       "  0.03381095826625824,\n",
       "  0.03360532596707344,\n",
       "  0.033370066434144974,\n",
       "  0.03320851922035217,\n",
       "  0.03297257050871849,\n",
       "  0.032814621925354004,\n",
       "  0.03259597718715668,\n",
       "  0.03238930553197861,\n",
       "  0.03222065046429634,\n",
       "  0.03204406425356865,\n",
       "  0.031817883253097534,\n",
       "  0.031638555228710175,\n",
       "  0.03140995651483536,\n",
       "  0.031210940331220627,\n",
       "  0.03105281852185726,\n",
       "  0.030856678262352943,\n",
       "  0.03067595697939396,\n",
       "  0.030436117202043533,\n",
       "  0.030286505818367004,\n",
       "  0.030077027156949043,\n",
       "  0.029886620119214058,\n",
       "  0.0297078900039196,\n",
       "  0.029575737193226814,\n",
       "  0.02934826910495758,\n",
       "  0.029150424525141716,\n",
       "  0.029005635529756546,\n",
       "  0.02885034866631031,\n",
       "  0.028692519292235374,\n",
       "  0.02859538421034813,\n",
       "  0.028356177732348442,\n",
       "  0.02817820943892002,\n",
       "  0.027993621304631233,\n",
       "  0.027819689363241196,\n",
       "  0.027616161853075027,\n",
       "  0.027456512674689293,\n",
       "  0.027329565957188606,\n",
       "  0.027138376608490944,\n",
       "  0.027011143043637276,\n",
       "  0.02685152366757393,\n",
       "  0.026684414595365524,\n",
       "  0.026555828750133514,\n",
       "  0.026404457166790962,\n",
       "  0.026247477158904076,\n",
       "  0.02608489617705345,\n",
       "  0.025907369330525398,\n",
       "  0.025761742144823074,\n",
       "  0.025617390871047974,\n",
       "  0.025490956380963326,\n",
       "  0.025326335802674294,\n",
       "  0.025151899084448814,\n",
       "  0.02503708004951477,\n",
       "  0.02493196353316307,\n",
       "  0.024793125689029694,\n",
       "  0.024602273479104042,\n",
       "  0.024497579783201218,\n",
       "  0.02432894892990589,\n",
       "  0.02417919784784317,\n",
       "  0.024038558825850487,\n",
       "  0.02389828860759735,\n",
       "  0.023781972005963326,\n",
       "  0.023626910522580147,\n",
       "  0.02348712459206581,\n",
       "  0.023441536352038383,\n",
       "  0.023256339132785797,\n",
       "  0.0231879074126482,\n",
       "  0.02298181876540184,\n",
       "  0.022844374179840088,\n",
       "  0.02273222990334034,\n",
       "  0.022572986781597137,\n",
       "  0.022478077560663223,\n",
       "  0.022352738305926323,\n",
       "  0.022211316972970963,\n",
       "  0.02208533324301243,\n",
       "  0.021971458569169044,\n",
       "  0.021844172850251198,\n",
       "  0.021734461188316345,\n",
       "  0.021640894934535027,\n",
       "  0.021473478525877,\n",
       "  0.021353306248784065,\n",
       "  0.021267270669341087,\n",
       "  0.0211319737136364,\n",
       "  0.021085351705551147,\n",
       "  0.02090730331838131,\n",
       "  0.020772462710738182,\n",
       "  0.02064496837556362,\n",
       "  0.02049875631928444,\n",
       "  0.020396217703819275,\n",
       "  0.020283982157707214,\n",
       "  0.020204544067382812,\n",
       "  0.020141802728176117,\n",
       "  0.019989771768450737,\n",
       "  0.019868921488523483,\n",
       "  0.01975403167307377,\n",
       "  0.019608773291110992,\n",
       "  0.01951598934829235,\n",
       "  0.019403062760829926,\n",
       "  0.019290262833237648,\n",
       "  0.019234996289014816,\n",
       "  0.019112693145871162,\n",
       "  0.018986575305461884,\n",
       "  0.018881630152463913,\n",
       "  0.01876669004559517,\n",
       "  0.018714845180511475,\n",
       "  0.018629157915711403,\n",
       "  0.01849868893623352,\n",
       "  0.01837192289531231,\n",
       "  0.018281854689121246,\n",
       "  0.01821214333176613,\n",
       "  0.01811271533370018,\n",
       "  0.01798327825963497,\n",
       "  0.017920274287462234,\n",
       "  0.017789114266633987,\n",
       "  0.01772650144994259,\n",
       "  0.017586907371878624,\n",
       "  0.017520615831017494,\n",
       "  0.0173957422375679,\n",
       "  0.01732030138373375,\n",
       "  0.01723872683942318,\n",
       "  0.017153311520814896,\n",
       "  0.017018642276525497,\n",
       "  0.017054466530680656,\n",
       "  0.01693015918135643,\n",
       "  0.016792943701148033,\n",
       "  0.016692042350769043,\n",
       "  0.016587303951382637,\n",
       "  0.016485389322042465,\n",
       "  0.016419734805822372,\n",
       "  0.01631566882133484,\n",
       "  0.0162131916731596,\n",
       "  0.016143659129738808,\n",
       "  0.016036752611398697,\n",
       "  0.015965359285473824,\n",
       "  0.015858815982937813,\n",
       "  0.015785077586770058,\n",
       "  0.015691079199314117,\n",
       "  0.015613000839948654,\n",
       "  0.015597833320498466,\n",
       "  0.015459488146007061,\n",
       "  0.015361315570771694,\n",
       "  0.015276741236448288,\n",
       "  0.015210088342428207,\n",
       "  0.015121561475098133,\n",
       "  0.01504283957183361,\n",
       "  0.014972487464547157,\n",
       "  0.014881870709359646,\n",
       "  0.01481342501938343,\n",
       "  0.014748909510672092,\n",
       "  0.014666588045656681,\n",
       "  0.014583843760192394,\n",
       "  0.014509388245642185,\n",
       "  0.014455473981797695,\n",
       "  0.014383349567651749,\n",
       "  0.014306711964309216,\n",
       "  0.014228972606360912,\n",
       "  0.01418496947735548,\n",
       "  0.014082342386245728,\n",
       "  0.01402122899889946,\n",
       "  0.013941101729869843,\n",
       "  0.013896258547902107,\n",
       "  0.013801521621644497,\n",
       "  0.013717127032577991,\n",
       "  0.013649151660501957,\n",
       "  0.013589409179985523,\n",
       "  0.013524198904633522,\n",
       "  0.01344455499202013,\n",
       "  0.013409308157861233,\n",
       "  0.013310092501342297,\n",
       "  0.013228941708803177,\n",
       "  0.013170422054827213,\n",
       "  0.013100218959152699,\n",
       "  0.01304310280829668,\n",
       "  0.013065304607152939,\n",
       "  0.012960883788764477,\n",
       "  0.012927583418786526,\n",
       "  0.012816435657441616,\n",
       "  0.012798882089555264,\n",
       "  0.012744764797389507,\n",
       "  0.01264660432934761,\n",
       "  0.012512155808508396,\n",
       "  0.012538941577076912,\n",
       "  0.012446454726159573,\n",
       "  0.012341120280325413,\n",
       "  0.012286229059100151,\n",
       "  0.012219369411468506,\n",
       "  0.012156958691775799,\n",
       "  0.012074200436472893,\n",
       "  0.012046302668750286,\n",
       "  0.011985850520431995,\n",
       "  0.011915785260498524,\n",
       "  0.011847537942230701,\n",
       "  0.011787651106715202,\n",
       "  0.011755591258406639,\n",
       "  0.011673985049128532,\n",
       "  0.011612707749009132,\n",
       "  0.01156358141452074,\n",
       "  0.011501604691147804,\n",
       "  0.011445888318121433,\n",
       "  0.011407829821109772,\n",
       "  0.011341046541929245,\n",
       "  0.011296833865344524,\n",
       "  0.011246823705732822,\n",
       "  0.011190179735422134,\n",
       "  0.01115152332931757,\n",
       "  0.011077705770730972,\n",
       "  0.011052079498767853,\n",
       "  0.010957326740026474,\n",
       "  0.01092229038476944,\n",
       "  0.010853271000087261,\n",
       "  0.010804091580212116,\n",
       "  0.010754765011370182,\n",
       "  0.010696757584810257,\n",
       "  0.010647997260093689,\n",
       "  0.01059913169592619,\n",
       "  0.01055681798607111,\n",
       "  0.010502326302230358,\n",
       "  0.010468981228768826,\n",
       "  0.010390914045274258,\n",
       "  0.010346011258661747,\n",
       "  0.010288468562066555,\n",
       "  0.010234239511191845,\n",
       "  0.010239900089800358,\n",
       "  0.010189667344093323,\n",
       "  0.010149512439966202,\n",
       "  0.010031938552856445,\n",
       "  0.010005430318415165,\n",
       "  0.009950428269803524,\n",
       "  0.009898737072944641,\n",
       "  0.009839045815169811,\n",
       "  0.009818247519433498,\n",
       "  0.009764944203197956,\n",
       "  0.009709268808364868,\n",
       "  0.009656562469899654,\n",
       "  0.009611609391868114,\n",
       "  0.009566541761159897,\n",
       "  0.009525357745587826,\n",
       "  0.009490271098911762,\n",
       "  0.009435933083295822,\n",
       "  0.009396901354193687,\n",
       "  0.009351705200970173,\n",
       "  0.009311091154813766,\n",
       "  0.009253459051251411,\n",
       "  0.009210433810949326,\n",
       "  0.009164041839540005,\n",
       "  0.009117490611970425,\n",
       "  0.009074938483536243,\n",
       "  0.009039681404829025,\n",
       "  0.008987246081233025,\n",
       "  0.008949640206992626,\n",
       "  0.008897197432816029,\n",
       "  0.008869550190865993,\n",
       "  0.008820709772408009,\n",
       "  0.008770722895860672,\n",
       "  0.008742560632526875,\n",
       "  0.008703360334038734,\n",
       "  0.008659766986966133,\n",
       "  0.008615667000412941,\n",
       "  0.008572746068239212,\n",
       "  0.008540602400898933,\n",
       "  0.008510985411703587,\n",
       "  0.008460466749966145,\n",
       "  0.008416826836764812,\n",
       "  0.008375574834644794,\n",
       "  0.008338959887623787,\n",
       "  0.008296120911836624,\n",
       "  0.008257911540567875,\n",
       "  0.00822376273572445,\n",
       "  0.008191176690161228,\n",
       "  0.008145276457071304,\n",
       "  0.008117171004414558,\n",
       "  0.008060441352427006,\n",
       "  0.008026805706322193,\n",
       "  0.00798975583165884,\n",
       "  0.007950704544782639,\n",
       "  0.00791081041097641,\n",
       "  0.007878218777477741,\n",
       "  0.0078474972397089,\n",
       "  0.00781670119613409,\n",
       "  0.0077918157912790775,\n",
       "  0.0077348691411316395,\n",
       "  0.007698385510593653,\n",
       "  0.007673343177884817,\n",
       "  0.007627544458955526,\n",
       "  0.007593026850372553,\n",
       "  0.007552931550890207,\n",
       "  0.007527110632508993,\n",
       "  0.007489622104912996,\n",
       "  0.007456493563950062,\n",
       "  0.007422079797834158,\n",
       "  0.007390919141471386,\n",
       "  0.007394779939204454,\n",
       "  0.0073389699682593346,\n",
       "  0.007305329665541649,\n",
       "  0.007250535301864147,\n",
       "  0.007213297300040722,\n",
       "  0.007182353176176548,\n",
       "  0.007153952494263649,\n",
       "  0.007114398293197155,\n",
       "  0.007085306104272604,\n",
       "  0.00705365277826786,\n",
       "  0.007024132180958986,\n",
       "  0.006991676986217499,\n",
       "  0.006961589213460684,\n",
       "  0.006933705415576696,\n",
       "  0.006893898826092482,\n",
       "  0.006862365175038576,\n",
       "  0.0068326531909406185,\n",
       "  0.006800901610404253,\n",
       "  0.006770396139472723,\n",
       "  0.006745256017893553,\n",
       "  0.006709547713398933,\n",
       "  0.006682077422738075,\n",
       "  0.0066567654721438885,\n",
       "  0.006635401397943497,\n",
       "  0.006595272570848465,\n",
       "  0.006557273678481579,\n",
       "  0.006532769650220871],\n",
       " 'accuracy': [0.5751072764396667,\n",
       "  0.6051502227783203,\n",
       "  0.6523604989051819,\n",
       "  0.6866952776908875,\n",
       "  0.725321888923645,\n",
       "  0.7381974458694458,\n",
       "  0.7467811107635498,\n",
       "  0.7510729432106018,\n",
       "  0.7639485001564026,\n",
       "  0.7639485001564026,\n",
       "  0.7725321650505066,\n",
       "  0.7768240571022034,\n",
       "  0.7768240571022034,\n",
       "  0.7811158895492554,\n",
       "  0.7811158895492554,\n",
       "  0.7811158895492554,\n",
       "  0.7811158895492554,\n",
       "  0.7854077219963074,\n",
       "  0.7854077219963074,\n",
       "  0.7854077219963074,\n",
       "  0.7854077219963074,\n",
       "  0.7896995544433594,\n",
       "  0.7896995544433594,\n",
       "  0.7896995544433594,\n",
       "  0.7896995544433594,\n",
       "  0.7896995544433594,\n",
       "  0.7896995544433594,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.7939913868904114,\n",
       "  0.8626609444618225,\n",
       "  0.9141631126403809,\n",
       "  0.9141631126403809,\n",
       "  0.9141631126403809,\n",
       "  0.9184549450874329,\n",
       "  0.9227467775344849,\n",
       "  0.9270386099815369,\n",
       "  0.9270386099815369,\n",
       "  0.9313304424285889,\n",
       "  0.9313304424285889,\n",
       "  0.9356223344802856,\n",
       "  0.9399141669273376,\n",
       "  0.9399141669273376,\n",
       "  0.9442059993743896,\n",
       "  0.9484978318214417,\n",
       "  0.9527897238731384,\n",
       "  0.9656652212142944,\n",
       "  0.9699570536613464,\n",
       "  0.9699570536613464,\n",
       "  0.9699570536613464,\n",
       "  0.9699570536613464,\n",
       "  0.9699570536613464,\n",
       "  0.9699570536613464,\n",
       "  0.9699570536613464,\n",
       "  0.9699570536613464,\n",
       "  0.9699570536613464,\n",
       "  0.9699570536613464,\n",
       "  0.9742489457130432,\n",
       "  0.9742489457130432,\n",
       "  0.9828326106071472,\n",
       "  0.9828326106071472,\n",
       "  0.9828326106071472,\n",
       "  0.9871244430541992,\n",
       "  0.9871244430541992,\n",
       "  0.9871244430541992,\n",
       "  0.9871244430541992,\n",
       "  0.9871244430541992,\n",
       "  0.9871244430541992,\n",
       "  0.9871244430541992,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.991416335105896,\n",
       "  0.991416335105896,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  0.995708167552948,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " 'val_loss': [0.9916831254959106,\n",
       "  0.9691751003265381,\n",
       "  0.9478479623794556,\n",
       "  0.9264240264892578,\n",
       "  0.9043600559234619,\n",
       "  0.8816307783126831,\n",
       "  0.8568108081817627,\n",
       "  0.8296011090278625,\n",
       "  0.801613450050354,\n",
       "  0.771531343460083,\n",
       "  0.7397307753562927,\n",
       "  0.7060255408287048,\n",
       "  0.6727039217948914,\n",
       "  0.6411589980125427,\n",
       "  0.6098341941833496,\n",
       "  0.5821975469589233,\n",
       "  0.5574343204498291,\n",
       "  0.5350006222724915,\n",
       "  0.5148641467094421,\n",
       "  0.496410608291626,\n",
       "  0.47924137115478516,\n",
       "  0.4633428156375885,\n",
       "  0.4481266438961029,\n",
       "  0.4346809387207031,\n",
       "  0.42290306091308594,\n",
       "  0.41176143288612366,\n",
       "  0.4016726613044739,\n",
       "  0.3927003741264343,\n",
       "  0.3835279047489166,\n",
       "  0.3756508529186249,\n",
       "  0.3674500286579132,\n",
       "  0.35932308435440063,\n",
       "  0.3521234095096588,\n",
       "  0.345180869102478,\n",
       "  0.33828186988830566,\n",
       "  0.3332657516002655,\n",
       "  0.3271152973175049,\n",
       "  0.3206785321235657,\n",
       "  0.3148052990436554,\n",
       "  0.30999675393104553,\n",
       "  0.3042115271091461,\n",
       "  0.2988252639770508,\n",
       "  0.2945805788040161,\n",
       "  0.2893308401107788,\n",
       "  0.2833414077758789,\n",
       "  0.27788639068603516,\n",
       "  0.27250128984451294,\n",
       "  0.2671280801296234,\n",
       "  0.26219165325164795,\n",
       "  0.2576427459716797,\n",
       "  0.2532537281513214,\n",
       "  0.24924543499946594,\n",
       "  0.2451508343219757,\n",
       "  0.24160192906856537,\n",
       "  0.23771259188652039,\n",
       "  0.23345527052879333,\n",
       "  0.229905903339386,\n",
       "  0.22673611342906952,\n",
       "  0.22339758276939392,\n",
       "  0.22090598940849304,\n",
       "  0.21775583922863007,\n",
       "  0.21460391581058502,\n",
       "  0.21143631637096405,\n",
       "  0.208699569106102,\n",
       "  0.2059583067893982,\n",
       "  0.20324984192848206,\n",
       "  0.20075157284736633,\n",
       "  0.1979036182165146,\n",
       "  0.1955336183309555,\n",
       "  0.19335313141345978,\n",
       "  0.19111284613609314,\n",
       "  0.1889350861310959,\n",
       "  0.1869671642780304,\n",
       "  0.18458566069602966,\n",
       "  0.18230892717838287,\n",
       "  0.18015161156654358,\n",
       "  0.17826679348945618,\n",
       "  0.1763497143983841,\n",
       "  0.17510053515434265,\n",
       "  0.17330938577651978,\n",
       "  0.17123332619667053,\n",
       "  0.1691480576992035,\n",
       "  0.16701486706733704,\n",
       "  0.16508273780345917,\n",
       "  0.163523331284523,\n",
       "  0.16202300786972046,\n",
       "  0.16023403406143188,\n",
       "  0.1582435667514801,\n",
       "  0.15642231702804565,\n",
       "  0.15484219789505005,\n",
       "  0.15336276590824127,\n",
       "  0.1516013741493225,\n",
       "  0.15002208948135376,\n",
       "  0.14852458238601685,\n",
       "  0.1466587781906128,\n",
       "  0.14519600570201874,\n",
       "  0.14373686909675598,\n",
       "  0.14219309389591217,\n",
       "  0.14078937470912933,\n",
       "  0.139388307929039,\n",
       "  0.1379140317440033,\n",
       "  0.13638068735599518,\n",
       "  0.1351252645254135,\n",
       "  0.1338309794664383,\n",
       "  0.1324557363986969,\n",
       "  0.13093173503875732,\n",
       "  0.1299820989370346,\n",
       "  0.12873132526874542,\n",
       "  0.12756600975990295,\n",
       "  0.12599585950374603,\n",
       "  0.12452743202447891,\n",
       "  0.12325488775968552,\n",
       "  0.12188517302274704,\n",
       "  0.12090498954057693,\n",
       "  0.1198251023888588,\n",
       "  0.11873786896467209,\n",
       "  0.11767193675041199,\n",
       "  0.11632199585437775,\n",
       "  0.11522354930639267,\n",
       "  0.11424707621335983,\n",
       "  0.1133098378777504,\n",
       "  0.11233868449926376,\n",
       "  0.11128091812133789,\n",
       "  0.11040113121271133,\n",
       "  0.10951635241508484,\n",
       "  0.10846603661775589,\n",
       "  0.10749635100364685,\n",
       "  0.10646601766347885,\n",
       "  0.10596363991498947,\n",
       "  0.10501732677221298,\n",
       "  0.10400880873203278,\n",
       "  0.10294097661972046,\n",
       "  0.1019362062215805,\n",
       "  0.10117312520742416,\n",
       "  0.1002037301659584,\n",
       "  0.09909301996231079,\n",
       "  0.09820012003183365,\n",
       "  0.09736493974924088,\n",
       "  0.09668833017349243,\n",
       "  0.09591291099786758,\n",
       "  0.09500919282436371,\n",
       "  0.09407956898212433,\n",
       "  0.0931730717420578,\n",
       "  0.09217119216918945,\n",
       "  0.09127209335565567,\n",
       "  0.09048791229724884,\n",
       "  0.08970866352319717,\n",
       "  0.08900029212236404,\n",
       "  0.08826665580272675,\n",
       "  0.0875617116689682,\n",
       "  0.0870000347495079,\n",
       "  0.08628429472446442,\n",
       "  0.08552858233451843,\n",
       "  0.08472288399934769,\n",
       "  0.084003746509552,\n",
       "  0.08324415981769562,\n",
       "  0.0825500413775444,\n",
       "  0.08187933266162872,\n",
       "  0.08109316229820251,\n",
       "  0.08038942515850067,\n",
       "  0.07971895486116409,\n",
       "  0.07913235574960709,\n",
       "  0.07849479466676712,\n",
       "  0.07796850800514221,\n",
       "  0.07734362781047821,\n",
       "  0.07672005891799927,\n",
       "  0.07604330033063889,\n",
       "  0.07535490393638611,\n",
       "  0.07472418993711472,\n",
       "  0.07413619756698608,\n",
       "  0.073504239320755,\n",
       "  0.07290391623973846,\n",
       "  0.0723181962966919,\n",
       "  0.07173418253660202,\n",
       "  0.0711631253361702,\n",
       "  0.07057995349168777,\n",
       "  0.06999059021472931,\n",
       "  0.06941276043653488,\n",
       "  0.0688757449388504,\n",
       "  0.06836996227502823,\n",
       "  0.06778392195701599,\n",
       "  0.06719193607568741,\n",
       "  0.0667305663228035,\n",
       "  0.06616027653217316,\n",
       "  0.0656706839799881,\n",
       "  0.06516486406326294,\n",
       "  0.0646049752831459,\n",
       "  0.06410209834575653,\n",
       "  0.063544362783432,\n",
       "  0.06307918578386307,\n",
       "  0.062602698802948,\n",
       "  0.06213649734854698,\n",
       "  0.06171243265271187,\n",
       "  0.061217956244945526,\n",
       "  0.060749899595975876,\n",
       "  0.0603061206638813,\n",
       "  0.05979950353503227,\n",
       "  0.05935347452759743,\n",
       "  0.05890440568327904,\n",
       "  0.05845671147108078,\n",
       "  0.05801299586892128,\n",
       "  0.05761829391121864,\n",
       "  0.05718497931957245,\n",
       "  0.056705575436353683,\n",
       "  0.05629735067486763,\n",
       "  0.0558939129114151,\n",
       "  0.05554252117872238,\n",
       "  0.05515366047620773,\n",
       "  0.05481782928109169,\n",
       "  0.05441594496369362,\n",
       "  0.0539865680038929,\n",
       "  0.05353941768407822,\n",
       "  0.05306423082947731,\n",
       "  0.05263158679008484,\n",
       "  0.052236657589673996,\n",
       "  0.05186237394809723,\n",
       "  0.051506660878658295,\n",
       "  0.05115515738725662,\n",
       "  0.05079641938209534,\n",
       "  0.05040362849831581,\n",
       "  0.05003641918301582,\n",
       "  0.04967551305890083,\n",
       "  0.04929326847195625,\n",
       "  0.04891620948910713,\n",
       "  0.04855552315711975,\n",
       "  0.04818125069141388,\n",
       "  0.04782693088054657,\n",
       "  0.047481730580329895,\n",
       "  0.047152843326330185,\n",
       "  0.046822257339954376,\n",
       "  0.0464952178299427,\n",
       "  0.04618753120303154,\n",
       "  0.045840565115213394,\n",
       "  0.04552432894706726,\n",
       "  0.04518081620335579,\n",
       "  0.04487782344222069,\n",
       "  0.04455150291323662,\n",
       "  0.04420287907123566,\n",
       "  0.04390445351600647,\n",
       "  0.04361730068922043,\n",
       "  0.04337575286626816,\n",
       "  0.04309734329581261,\n",
       "  0.04280630499124527,\n",
       "  0.042491186410188675,\n",
       "  0.0421941764652729,\n",
       "  0.041922979056835175,\n",
       "  0.04164336994290352,\n",
       "  0.04137427359819412,\n",
       "  0.041080862283706665,\n",
       "  0.040794115513563156,\n",
       "  0.04051073640584946,\n",
       "  0.04025088623166084,\n",
       "  0.039939165115356445,\n",
       "  0.03964822366833687,\n",
       "  0.03938042372465134,\n",
       "  0.03913521394133568,\n",
       "  0.03888469934463501,\n",
       "  0.03864001855254173,\n",
       "  0.03838248550891876,\n",
       "  0.03810557350516319,\n",
       "  0.03785201534628868,\n",
       "  0.03759443014860153,\n",
       "  0.03732560575008392,\n",
       "  0.03706830367445946,\n",
       "  0.0368347205221653,\n",
       "  0.03659758344292641,\n",
       "  0.03634683042764664,\n",
       "  0.03611178323626518,\n",
       "  0.035884443670511246,\n",
       "  0.03565694019198418,\n",
       "  0.035429444164037704,\n",
       "  0.0351981557905674,\n",
       "  0.03497198969125748,\n",
       "  0.03475392609834671,\n",
       "  0.03453970327973366,\n",
       "  0.03427828475832939,\n",
       "  0.03403635323047638,\n",
       "  0.03380930796265602,\n",
       "  0.033592093735933304,\n",
       "  0.03338310867547989,\n",
       "  0.03317289054393768,\n",
       "  0.032973334193229675,\n",
       "  0.03276978060603142,\n",
       "  0.032569531351327896,\n",
       "  0.03236386924982071,\n",
       "  0.03216306120157242,\n",
       "  0.031967196613550186,\n",
       "  0.03177696466445923,\n",
       "  0.031590595841407776,\n",
       "  0.031399961560964584,\n",
       "  0.031205059960484505,\n",
       "  0.031032554805278778,\n",
       "  0.030844468623399734,\n",
       "  0.030642762780189514,\n",
       "  0.030446233227849007,\n",
       "  0.030256856232881546,\n",
       "  0.030078379437327385,\n",
       "  0.02990640327334404,\n",
       "  0.029711950570344925,\n",
       "  0.029523925855755806,\n",
       "  0.029325181618332863,\n",
       "  0.02912713959813118,\n",
       "  0.02893930859863758,\n",
       "  0.02876175194978714,\n",
       "  0.02859642542898655,\n",
       "  0.0284205861389637,\n",
       "  0.028254980221390724,\n",
       "  0.028081683441996574,\n",
       "  0.027919165790081024,\n",
       "  0.027748242020606995,\n",
       "  0.027564020827412605,\n",
       "  0.027396732941269875,\n",
       "  0.027235301211476326,\n",
       "  0.027081023901700974,\n",
       "  0.026918428018689156,\n",
       "  0.02675963193178177,\n",
       "  0.02660473622381687,\n",
       "  0.026444582268595695,\n",
       "  0.026285579428076744,\n",
       "  0.026134803891181946,\n",
       "  0.025984259322285652,\n",
       "  0.025836074724793434,\n",
       "  0.025691961869597435,\n",
       "  0.025550832971930504,\n",
       "  0.025405921041965485,\n",
       "  0.02526009827852249,\n",
       "  0.02511569671332836,\n",
       "  0.024965403601527214,\n",
       "  0.02481498196721077,\n",
       "  0.024658728390932083,\n",
       "  0.02451777271926403,\n",
       "  0.024377023801207542,\n",
       "  0.024236902594566345,\n",
       "  0.02409922331571579,\n",
       "  0.02396339364349842,\n",
       "  0.023836277425289154,\n",
       "  0.023706557229161263,\n",
       "  0.023580003529787064,\n",
       "  0.023446736857295036,\n",
       "  0.023312358185648918,\n",
       "  0.02317919209599495,\n",
       "  0.0230376236140728,\n",
       "  0.022901227697730064,\n",
       "  0.022761845961213112,\n",
       "  0.022638799622654915,\n",
       "  0.022505128756165504,\n",
       "  0.022371850907802582,\n",
       "  0.022249236702919006,\n",
       "  0.022132564336061478,\n",
       "  0.022017702460289,\n",
       "  0.02189870923757553,\n",
       "  0.021774766966700554,\n",
       "  0.021659141406416893,\n",
       "  0.021545181050896645,\n",
       "  0.021426649764180183,\n",
       "  0.021309442818164825,\n",
       "  0.021199267357587814,\n",
       "  0.021086400374770164,\n",
       "  0.02097667008638382,\n",
       "  0.020863033831119537,\n",
       "  0.02074904926121235,\n",
       "  0.02064034715294838,\n",
       "  0.020527778193354607,\n",
       "  0.02041763998568058,\n",
       "  0.020297732204198837,\n",
       "  0.020176289603114128,\n",
       "  0.02005940116941929,\n",
       "  0.0199433546513319,\n",
       "  0.019833723083138466,\n",
       "  0.019729072228074074,\n",
       "  0.019627254456281662,\n",
       "  0.01952776312828064,\n",
       "  0.019420385360717773,\n",
       "  0.019310489296913147,\n",
       "  0.019203785806894302,\n",
       "  0.01910153217613697,\n",
       "  0.019000202417373657,\n",
       "  0.018901702016592026,\n",
       "  0.018792062997817993,\n",
       "  0.01869157887995243,\n",
       "  0.01859522983431816,\n",
       "  0.018497852608561516,\n",
       "  0.01838931441307068,\n",
       "  0.018283730372786522,\n",
       "  0.01818208396434784,\n",
       "  0.018090959638357162,\n",
       "  0.017996972426772118,\n",
       "  0.017901204526424408,\n",
       "  0.01780892163515091,\n",
       "  0.01771422103047371,\n",
       "  0.017617428675293922,\n",
       "  0.017525166273117065,\n",
       "  0.017431160435080528,\n",
       "  0.017343241721391678,\n",
       "  0.017259277403354645,\n",
       "  0.017165619879961014,\n",
       "  0.01707560569047928,\n",
       "  0.01699044182896614,\n",
       "  0.016904408112168312,\n",
       "  0.01681346818804741,\n",
       "  0.016728149726986885,\n",
       "  0.01664191670715809,\n",
       "  0.016565266996622086,\n",
       "  0.016486776992678642,\n",
       "  0.016398392617702484,\n",
       "  0.01630733534693718,\n",
       "  0.016219375655055046,\n",
       "  0.016134506091475487,\n",
       "  0.016049403697252274,\n",
       "  0.01596173644065857,\n",
       "  0.015876006335020065,\n",
       "  0.0157889686524868,\n",
       "  0.01569974422454834,\n",
       "  0.01561617013067007,\n",
       "  0.015538078732788563,\n",
       "  0.015455094166100025,\n",
       "  0.015372442081570625,\n",
       "  0.015293069183826447,\n",
       "  0.015208964236080647,\n",
       "  0.015125559642910957,\n",
       "  0.01504654809832573,\n",
       "  0.01497135404497385,\n",
       "  0.014893327839672565,\n",
       "  0.014817736111581326,\n",
       "  0.014732006005942822,\n",
       "  0.014654635451734066,\n",
       "  0.014583625830709934,\n",
       "  0.014509872533380985,\n",
       "  0.014439273625612259,\n",
       "  0.0143666947260499,\n",
       "  0.014297009445726871,\n",
       "  0.014226525090634823,\n",
       "  0.014150951988995075,\n",
       "  0.014078008942306042,\n",
       "  0.0140067795291543,\n",
       "  0.013940033502876759,\n",
       "  0.0138693293556571,\n",
       "  0.013798555359244347,\n",
       "  0.013729158788919449,\n",
       "  0.013659528456628323,\n",
       "  0.013589169830083847,\n",
       "  0.013516267761588097,\n",
       "  0.013448950834572315,\n",
       "  0.013381005264818668,\n",
       "  0.013318083249032497,\n",
       "  0.013254577293992043,\n",
       "  0.013189933262765408,\n",
       "  0.013121495023369789,\n",
       "  0.013051471672952175,\n",
       "  0.012987193651497364,\n",
       "  0.012925142422318459,\n",
       "  0.01286375056952238,\n",
       "  0.012800737284123898,\n",
       "  0.012736141681671143,\n",
       "  0.012668918818235397,\n",
       "  0.012605979107320309,\n",
       "  0.012547197751700878,\n",
       "  0.012490056455135345,\n",
       "  0.012430626899003983,\n",
       "  0.012375670485198498,\n",
       "  0.012315784581005573,\n",
       "  0.012253654189407825,\n",
       "  0.012195369228720665,\n",
       "  0.012138498947024345,\n",
       "  0.01208521705120802,\n",
       "  0.012028812430799007,\n",
       "  0.011969499289989471,\n",
       "  0.011906457133591175,\n",
       "  0.01184949278831482,\n",
       "  0.011791756376624107,\n",
       "  0.0117344930768013,\n",
       "  0.01167911384254694,\n",
       "  0.011623702943325043,\n",
       "  0.011568342335522175,\n",
       "  0.011508521623909473,\n",
       "  0.011453071609139442,\n",
       "  0.011402761563658714,\n",
       "  0.011349103413522243,\n",
       "  0.01129743829369545,\n",
       "  0.011246169917285442,\n",
       "  0.011197155341506004,\n",
       "  0.011142943985760212,\n",
       "  0.011094274930655956,\n",
       "  0.0110428212210536,\n",
       "  0.010989430360496044,\n",
       "  0.010936751030385494,\n",
       "  0.010883647948503494,\n",
       "  0.010831582359969616,\n",
       "  0.010778295807540417,\n",
       "  0.010728716850280762,\n",
       "  0.010679160244762897,\n",
       "  0.010626128874719143,\n",
       "  0.01057299505919218,\n",
       "  0.01051983330398798,\n",
       "  0.010471850633621216,\n",
       "  0.010424625128507614,\n",
       "  0.01037758868187666,\n",
       "  0.010326913557946682,\n",
       "  0.010278296656906605,\n",
       "  0.010230247862637043,\n",
       "  0.010181987658143044,\n",
       "  0.010131645947694778,\n",
       "  0.010083824396133423,\n",
       "  0.010031073354184628,\n",
       "  0.009981931187212467,\n",
       "  0.009935458190739155,\n",
       "  0.009890574030578136,\n",
       "  0.009847570210695267,\n",
       "  0.009803513996303082,\n",
       "  0.009757166728377342,\n",
       "  0.009708113968372345,\n",
       "  0.009660185314714909,\n",
       "  0.009614790789783001,\n",
       "  0.0095717404037714,\n",
       "  0.009527573361992836,\n",
       "  0.009486174210906029,\n",
       "  0.00944592710584402,\n",
       "  0.00939958170056343,\n",
       "  0.009355440735816956,\n",
       "  0.009312317706644535,\n",
       "  0.009270106442272663,\n",
       "  0.009225660003721714,\n",
       "  0.00918158981949091,\n",
       "  0.009140578098595142,\n",
       "  0.009095295332372189,\n",
       "  0.009050906635820866,\n",
       "  0.009008661843836308,\n",
       "  0.00896570086479187,\n",
       "  0.008922531269490719,\n",
       "  0.008879581466317177,\n",
       "  0.008837921544909477,\n",
       "  0.008794337511062622,\n",
       "  0.008753971196711063,\n",
       "  0.008713719435036182,\n",
       "  0.008675306104123592,\n",
       "  0.008635804988443851,\n",
       "  0.00859973393380642,\n",
       "  0.008561573922634125,\n",
       "  0.008524838835000992,\n",
       "  0.008486627601087093,\n",
       "  0.008448419161140919,\n",
       "  0.008409766480326653,\n",
       "  0.008370728231966496,\n",
       "  0.008333771489560604,\n",
       "  0.008297063410282135,\n",
       "  0.008256753906607628,\n",
       "  0.00821734219789505,\n",
       "  0.008178766816854477,\n",
       "  0.008141100406646729,\n",
       "  0.008099749684333801,\n",
       "  0.008062783628702164,\n",
       "  0.008025231771171093,\n",
       "  0.007989001460373402,\n",
       "  0.00795284379273653,\n",
       "  0.007914903573691845,\n",
       "  0.007877478376030922,\n",
       "  0.007842625491321087,\n",
       "  0.007807887624949217,\n",
       "  0.007771662902086973,\n",
       "  0.00773937813937664,\n",
       "  0.007705614436417818,\n",
       "  0.00766957039013505,\n",
       "  0.007634797133505344,\n",
       "  0.007600143551826477,\n",
       "  0.0075682164169847965,\n",
       "  0.007534949574619532,\n",
       "  0.0074991500005126,\n",
       "  0.007466520182788372,\n",
       "  0.007433716673403978,\n",
       "  0.00740137230604887,\n",
       "  0.007369456347078085,\n",
       "  0.00733658391982317,\n",
       "  0.007305765058845282,\n",
       "  0.007274138741195202,\n",
       "  0.007241909392178059,\n",
       "  0.0072081745602190495,\n",
       "  0.007173957768827677,\n",
       "  0.007140479050576687,\n",
       "  0.007107914891093969,\n",
       "  0.007076718844473362,\n",
       "  0.007046208251267672,\n",
       "  0.007013329770416021,\n",
       "  0.00698392279446125,\n",
       "  0.0069520859979093075,\n",
       "  0.00692240335047245,\n",
       "  0.006892538629472256,\n",
       "  0.006862614769488573,\n",
       "  0.006832869723439217,\n",
       "  0.006802164483815432,\n",
       "  0.006771204527467489,\n",
       "  0.006742837373167276,\n",
       "  0.006713066715747118,\n",
       "  0.006684799678623676,\n",
       "  0.006657186895608902,\n",
       "  0.0066275931894779205,\n",
       "  0.006599607411772013,\n",
       "  0.006570009049028158,\n",
       "  0.006536648143082857,\n",
       "  0.006505839992314577,\n",
       "  0.006476934999227524],\n",
       " 'val_accuracy': [0.6200000047683716,\n",
       "  0.6700000166893005,\n",
       "  0.7300000190734863,\n",
       "  0.7300000190734863,\n",
       "  0.7300000190734863,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.7699999809265137,\n",
       "  0.7699999809265137,\n",
       "  0.7699999809265137,\n",
       "  0.7699999809265137,\n",
       "  0.7699999809265137,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.7799999713897705,\n",
       "  0.8299999833106995,\n",
       "  0.9399999976158142,\n",
       "  0.949999988079071,\n",
       "  0.949999988079071,\n",
       "  0.949999988079071,\n",
       "  0.949999988079071,\n",
       "  0.9599999785423279,\n",
       "  0.9599999785423279,\n",
       "  0.9599999785423279,\n",
       "  0.9700000286102295,\n",
       "  0.9700000286102295,\n",
       "  0.9700000286102295,\n",
       "  0.9800000190734863,\n",
       "  0.9800000190734863,\n",
       "  0.9900000095367432,\n",
       "  0.9900000095367432,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e99e1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6604022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = model.history.history['loss']\n",
    "# sns.lineplot(x=range(len(loss)),y=loss)\n",
    "# plt.title(\"Training Loss per Epoch\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a372870e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08a30d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score = model.evaluate(X_train,y_train,verbose=0)\n",
    "test_score = model.evaluate(X_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c820ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.006506654433906078, 1.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "046cf10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.006476934999227524, 1.0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa86d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "test_predictions = pd.DataFrame(test_predictions, columns = y.columns)\n",
    "# test_predictions = pd.Series(test_predictions.reshape(test_predictions.shape[0],))\n",
    "# test_predictions = test_predictions.apply(lambda x: 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fce9381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adelie</th>\n",
       "      <th>Chinstrap</th>\n",
       "      <th>Gentoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.694356e-02</td>\n",
       "      <td>9.701512e-01</td>\n",
       "      <td>1.290517e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.694356e-02</td>\n",
       "      <td>9.701512e-01</td>\n",
       "      <td>1.290517e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.841318e-14</td>\n",
       "      <td>4.088413e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.668745e-07</td>\n",
       "      <td>1.969669e-06</td>\n",
       "      <td>9.999979e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.999990e-01</td>\n",
       "      <td>4.011033e-07</td>\n",
       "      <td>6.252506e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.819667e-07</td>\n",
       "      <td>2.166381e-06</td>\n",
       "      <td>9.999976e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.273508e-14</td>\n",
       "      <td>3.692045e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.024651e-17</td>\n",
       "      <td>1.443394e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>9.934213e-08</td>\n",
       "      <td>1.113611e-06</td>\n",
       "      <td>9.999988e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.111459e-07</td>\n",
       "      <td>3.907308e-06</td>\n",
       "      <td>9.999958e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Adelie     Chinstrap        Gentoo\n",
       "0   1.694356e-02  9.701512e-01  1.290517e-02\n",
       "1   1.694356e-02  9.701512e-01  1.290517e-02\n",
       "2   1.000000e+00  4.841318e-14  4.088413e-12\n",
       "3   1.668745e-07  1.969669e-06  9.999979e-01\n",
       "4   9.999990e-01  4.011033e-07  6.252506e-07\n",
       "..           ...           ...           ...\n",
       "95  1.819667e-07  2.166381e-06  9.999976e-01\n",
       "96  1.000000e+00  4.273508e-14  3.692045e-12\n",
       "97  1.000000e+00  2.024651e-17  1.443394e-14\n",
       "98  9.934213e-08  1.113611e-06  9.999988e-01\n",
       "99  3.111459e-07  3.907308e-06  9.999958e-01\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2db6deff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.970151\n",
       "1     0.970151\n",
       "2     1.000000\n",
       "3     0.999998\n",
       "4     0.999999\n",
       "        ...   \n",
       "95    0.999998\n",
       "96    1.000000\n",
       "97    1.000000\n",
       "98    0.999999\n",
       "99    0.999996\n",
       "Length: 100, dtype: float32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions.max(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2edfecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3fade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff700674",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(y_test).reset_index(drop = True)\n",
    "pred_df['Model_Predictions'] = test_predictions\n",
    "pred_df.columns = ['Test_Y','Model_Predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e27a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f6a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(pred_df['Test_Y'],pred_df['Model_Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(pred_df['Test_Y'],pred_df['Model_Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f82c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(pred_df['Test_Y'],pred_df['Model_Predictions'])\n",
    "metrics.ConfusionMatrixDisplay(confusion_matrix = cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918739a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(pred_df['Test_Y'],pred_df['Model_Predictions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3810cb8b",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317bee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "\n",
    "model.add(Dense(units=8,activation='relu'))\n",
    "\n",
    "model.add(Dense(units=4,activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(units=3,activation='softmax'))\n",
    "\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db137625",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c493634",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1,\n",
    "          callbacks=[early_stop, board]\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac2fd3",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d49aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = model.history.history['loss']\n",
    "# sns.lineplot(x=range(len(loss)),y=loss)\n",
    "# plt.title(\"Training Loss per Epoch\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab16901",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score = model.evaluate(X_train,y_train,verbose=0)\n",
    "test_score = model.evaluate(X_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb878ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "test_predictions = pd.Series(test_predictions.reshape(test_predictions.shape[0],))\n",
    "test_predictions = test_predictions.apply(lambda x: 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(y_test).reset_index(drop = True)\n",
    "pred_df['Model_Predictions'] = test_predictions\n",
    "pred_df.columns = ['Test_Y','Model_Predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eaba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(pred_df['Test_Y'],pred_df['Model_Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c081bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(pred_df['Test_Y'],pred_df['Model_Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1313f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(pred_df['Test_Y'],pred_df['Model_Predictions'])\n",
    "metrics.ConfusionMatrixDisplay(confusion_matrix = cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd800da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(pred_df['Test_Y'],pred_df['Model_Predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c6ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cc2802d",
   "metadata": {},
   "source": [
    "## Adding in DropOut Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "\n",
    "model.add(Dense(units=8,activation='relu'))\n",
    "\n",
    "model.add(Dense(units=4,activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(units=3,activation='softmax'))\n",
    "\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1,\n",
    "          callbacks=[early_stop]\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b1a87",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8232e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = model.history.history['loss']\n",
    "# sns.lineplot(x=range(len(loss)),y=loss)\n",
    "# plt.title(\"Training Loss per Epoch\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac359a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0db3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score = model.evaluate(X_train,y_train,verbose=0)\n",
    "test_score = model.evaluate(X_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907593dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ce405",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "test_predictions = pd.Series(test_predictions.reshape(test_predictions.shape[0],))\n",
    "test_predictions = test_predictions.apply(lambda x: 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86213511",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(y_test).reset_index(drop = True)\n",
    "pred_df['Model_Predictions'] = test_predictions\n",
    "pred_df.columns = ['Test_Y','Model_Predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f368f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(pred_df['Test_Y'],pred_df['Model_Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(pred_df['Test_Y'],pred_df['Model_Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0046de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(pred_df['Test_Y'],pred_df['Model_Predictions'])\n",
    "metrics.ConfusionMatrixDisplay(confusion_matrix = cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec797da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(pred_df['Test_Y'],pred_df['Model_Predictions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f0f94",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98cae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff73b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf77465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# later_model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d7318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
